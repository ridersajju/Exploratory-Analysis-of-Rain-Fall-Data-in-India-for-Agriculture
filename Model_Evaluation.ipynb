{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e32abd64",
   "metadata": {},
   "source": [
    "# Model Evaluation for Rainfall Prediction\n",
    "\n",
    "This notebook evaluates the trained machine learning models using various evaluation metrics including:\n",
    "- Accuracy Score\n",
    "- Confusion Matrix\n",
    "- ROC-AUC Curve\n",
    "- Precision, Recall, and F1-Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a97ee5",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4b12ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5031eaef",
   "metadata": {},
   "source": [
    "## 2. Load the Test Data and Predictions\n",
    "\n",
    "Assuming you have already trained your models and have predictions saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd037613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your test data and predictions\n",
    "# y_test = your test labels\n",
    "# y_pred = predictions from your best model\n",
    "\n",
    "# For demonstration, assuming these are already available from training\n",
    "# You can load them from pickle files if needed:\n",
    "\n",
    "# y_test = pickle.load(open('y_test.pkl', 'rb'))\n",
    "# y_pred = pickle.load(open('y_pred.pkl', 'rb'))\n",
    "\n",
    "print(\"Test data loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8337a3a",
   "metadata": {},
   "source": [
    "## 3. Calculate Accuracy Scores for All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bc36df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy scores\n",
    "# Assuming p1_test through p7_test are predictions from different models\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"Model Accuracy Scores on Test Data\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "accuracy_scores = {\n",
    "    'xgboost': accuracy_score(y_test, p1_test),\n",
    "    'Random_Forest': accuracy_score(y_test, p2_test),\n",
    "    'svm': accuracy_score(y_test, p3_test),\n",
    "    'Dtree': accuracy_score(y_test, p4_test),\n",
    "    'GBM': accuracy_score(y_test, p5_test),\n",
    "    'log': accuracy_score(y_test, p6_test),\n",
    "    'KNN': accuracy_score(y_test, p7_test)\n",
    "}\n",
    "\n",
    "for model, score in accuracy_scores.items():\n",
    "    print(f\"{model}: {score}\")\n",
    "\n",
    "# Find the best model\n",
    "best_model = max(accuracy_scores, key=accuracy_scores.get)\n",
    "best_accuracy = accuracy_scores[best_model]\n",
    "\n",
    "print(f\"\\n✓ Best Model: {best_model} with Accuracy: {best_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2704a005",
   "metadata": {},
   "source": [
    "## 4. Confusion Matrix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b93182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix for the best model\n",
    "# Using Random Forest predictions (p2_test) as example\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, p2_test)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Extract values from confusion matrix\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "print(f\"\\nConfusion Matrix Components:\")\n",
    "print(f\"True Negative (TN): {tn}\")\n",
    "print(f\"False Positive (FP): {fp}\")\n",
    "print(f\"False Negative (FN): {fn}\")\n",
    "print(f\"True Positive (TP): {tp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc79093",
   "metadata": {},
   "source": [
    "## 5. Visualize Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba630ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a beautiful confusion matrix heatmap\n",
    "fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
    "ax.matshow(conf_matrix, alpha=0.3)\n",
    "\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        ax.text(x=j, y=i, s=conf_matrix[i, j], va='center', ha='center', size='xx-large')\n",
    "\n",
    "plt.xlabel('Predictions', fontsize=18)\n",
    "plt.ylabel('Actuals', fontsize=18)\n",
    "plt.title('Confusion Matrix', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8374bdf8",
   "metadata": {},
   "source": [
    "## 6. Calculate Detailed Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac0fa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate detailed metrics\n",
    "accuracy = accuracy_score(y_test, p2_test)\n",
    "precision = precision_score(y_test, p2_test, average='weighted', zero_division=0)\n",
    "recall = recall_score(y_test, p2_test, average='weighted', zero_division=0)\n",
    "f1 = f1_score(y_test, p2_test, average='weighted', zero_division=0)\n",
    "\n",
    "print(\"\\nDetailed Performance Metrics:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1-Score:  {f1:.4f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, p2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23619575",
   "metadata": {},
   "source": [
    "## 7. ROC-AUC Curve Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039a3005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC-AUC score\n",
    "# Note: Need probability predictions, not class predictions\n",
    "# y_pred_proba = model.predict_proba(x_test_scaled)[:, 1]\n",
    "\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"AUC Score: {auc:.4f}\")\n",
    "print(\"\\nROC Curve Information:\")\n",
    "print(f\"False Positive Rate (FPR) values: {fpr[:5]}...\")  # Show first 5 values\n",
    "print(f\"True Positive Rate (TPR) values: {tpr[:5]}...\")\n",
    "5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18515d30",
   "metadata": {},
   "source": [
    "## 8. Plot ROC-AUC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed8d9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(12, 10), dpi=80)\n",
    "plt.axis('scaled')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.title(\"AUC & ROC Curve\")\n",
    "plt.plot(fpr, tpr, 'v')\n",
    "plt.fill_between(fpr, tpr, facecolor='blue', alpha=0.8)\n",
    "plt.text(1, 0.05, f'AUC = {auc:.4f}', ha='right', fontsize=10, weight='bold', color='black')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cd8d92",
   "metadata": {},
   "source": [
    "## 9. Model Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5607e7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary dataframe of all models\n",
    "models_data = {\n",
    "    'Model': ['XGBoost', 'Random Forest', 'SVM', 'Decision Tree', 'GBM', 'Logistic Regression', 'KNN'],\n",
    "    'Accuracy': [accuracy_scores['xgboost'],\n",
    "                 accuracy_scores['Random_Forest'],\n",
    "                 accuracy_scores['svm'],\n",
    "                 accuracy_scores['Dtree'],\n",
    "                 accuracy_scores['GBM'],\n",
    "                 accuracy_scores['log'],\n",
    "                 accuracy_scores['KNN']]\n",
    "}\n",
    "\n",
    "models_df = pd.DataFrame(models_data)\n",
    "models_df = models_df.sort_values('Accuracy', ascending=False)\n",
    "\n",
    "print(\"\\nModel Comparison Summary:\")\n",
    "print(\"=\"*50)\n",
    "print(models_df.to_string(index=False))\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ed2793",
   "metadata": {},
   "source": [
    "## 10. Save the Best Model using Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5183dfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model (Random Forest in this example)\n",
    "import pickle\n",
    "\n",
    "# Save the trained model\n",
    "pickle.dump(Rand_Forest, open('Rainfall.pkl', 'wb'))  # Model saving\n",
    "print(\"✓ Model saved as 'Rainfall.pkl'\")\n",
    "\n",
    "# Save the scaler as well (important for preprocessing new data)\n",
    "pickle.dump(scaler, open('scale.pkl', 'wb'))  # Scaling the data\n",
    "print(\"✓ Scaler saved as 'scale.pkl'\")\n",
    "\n",
    "# Save encoder if you have one\n",
    "# pickle.dump(encoder, open('encoder.pkl', 'wb'))  # Encoder saving\n",
    "# pickle.dump(imputer, open('imputer.pkl', 'wb'))  # Imputer saving\n",
    "\n",
    "print(\"\\n✓ All models and preprocessing objects saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a218b568",
   "metadata": {},
   "source": [
    "## 11. Load and Test the Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150a2cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "loaded_model = pickle.load(open('Rainfall.pkl', 'rb'))\n",
    "loaded_scaler = pickle.load(open('scale.pkl', 'rb'))\n",
    "\n",
    "print(\"✓ Model loaded successfully from pickle file\")\n",
    "\n",
    "# Test the loaded model\n",
    "y_pred_loaded = loaded_model.predict(x_test_scaled)\n",
    "accuracy_loaded = accuracy_score(y_test, y_pred_loaded)\n",
    "\n",
    "print(f\"✓ Loaded Model Accuracy: {accuracy_loaded:.4f}\")\n",
    "print(f\"✓ Original Model Accuracy: {best_accuracy:.4f}\")\n",
    "print(f\"\\n✓ Model saved and loaded correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5430bab1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Accuracy Score**: Measures the ratio of correct predictions to total predictions\n",
    "2. **Confusion Matrix**: Shows True Positives, True Negatives, False Positives, and False Negatives\n",
    "3. **ROC-AUC Curve**: Evaluates the model's ability to distinguish between classes\n",
    "4. **Performance Metrics**: \n",
    "   - **Precision**: Out of predicted positive cases, how many were actually positive\n",
    "   - **Recall**: Out of actual positive cases, how many were correctly predicted\n",
    "   - **F1-Score**: Harmonic mean of precision and recall\n",
    "\n",
    "### Model Selection:\n",
    "The best model has been selected based on accuracy and saved as `Rainfall.pkl` for deployment in the Flask application."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
